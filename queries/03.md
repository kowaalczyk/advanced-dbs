# Wypisywanie publikacji danego autora

Treść zapytania sql znajdującego publikacje zadanego autora znajduje się w pliku `author_publications.sql`.
Pisząc je, starałem się aby wypisywane były dokładnie te same informacje co na przykładowej stronie.

Wyniki zapytań (w formacie csv) i pomiaru ich wydajności (w formacie yaml) znajdują się w folderze `results`.


## Środowisko

Ponieważ musiałem kolejny raz zmienić część schematu bazy (szczegóły opisane niżej) i ponownie wykonać całą konwersję danych,
wykonywałem eksperymenty na moim własnym laptopie (który jest w tym momencie szybszy niż maszyna `students`):
```
Model Name:	MacBook Pro
Model Identifier:	MacBookPro15,4
Processor Name:	Quad-Core Intel Core i7
Processor Speed:	1,7 GHz
Number of Processors:	1
Total Number of Cores:	4
L2 Cache (per Core):	256 KB
L3 Cache:	8 MB
Hyper-Threading Technology:	Enabled
Memory:	16 GB
```

Zamiast instalować bazę lokalnie, korzystałem z [oficjalnego obrazu dockera postgresql-9](https://hub.docker.com/_/postgres)
(ta sama wersja bazy co na maszynie `students`).

W czasie wykonywania eksperymentu laptop był umiarkowanie obciążony
(przeglądarka, IDE, docker - ok. 10% mocy procesora i 6 z 16 GB RAM w użyciu).

Mając na uwadze metodologie TPC-C i TPC-H, oraz fakt iż maszyna students jest w trakcie migracji danych 
(wyjątkowo duże obciążenie), wybrałem lokalne środowisko jako bardziej miarodajne miejsce do eksperymentów.


## Poprawa poprzednich zadań

Moje opóźnienie w oddawaniu tej części zadania wynikało z konieczności przepisania i ponownej konwersji wszystkich danych:
okazało się, że wcześniejszy format bazy (z zadania 02) nie był kompatybliny ze wszystkimi danymi, przez co znacząca cześć
zapytań kończyła się błędami. Poprawienie formatu (opisane niżej) spowodowało znaczące wydłużenie czasu dodawania danych
(pomimo asynchronicznych zapytań, oczekiwany czas wynosił ponad 10 godzin), więc zdecydowałem się przepisać cały skrypt
tak, aby operował jak najdłużej na plikach tekstowych (`xml_to_csv.py`, `format.sh`), 
następnie załadował je za pomocą polecenia `\copy` do bazy (`load.sh`)
i na koniec dodał indeksy (`post-load.sql`, pierwotnie były one dodawane w `init.sql` wykonywanym na samym początku).

Wszystkie pliki potrzebne do reprodukcji procesu ładowania (oprócz samych danych) znajdują się w folderze `loading`,
załączony `Makefile` pozwala na automatyczną reprodukcję procesu (co może zająć od 2 do 6 godzin).


### Zmiany schematu bazy

* strony publikacji przechowywane są jako `text`, nie `int4range`
* klucz główny relacji `publication` jest typu `varchar(80)` zamiast `char(60)` (większy i efektywniejszy jednocześnie)
* wszystkie inne pola typu `varchar` zostały zamienione na `text` (poza `isbn.isbn` i `person.orcid`)
* `isbn` nie jest unikalny w danych
* `publication.title` może być nullowalny

Nowy schemat można zobaczyć w pliku `loading/dblp-2019-11-03_12_49.pdf`.
